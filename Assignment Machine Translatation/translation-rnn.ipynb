{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8656885,"sourceType":"datasetVersion","datasetId":5186205},{"sourceId":8694740,"sourceType":"datasetVersion","datasetId":5214124}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport spacy\nimport torchtext\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nfrom torch import optim\nfrom torch.optim import Adam\n\nimport time\nimport math\n\nimport html\n\nfrom collections import Counter\nfrom string import punctuation\nfrom nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\nfrom itertools import product","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:16.086676Z","iopub.execute_input":"2024-06-21T15:27:16.087283Z","iopub.status.idle":"2024-06-21T15:27:26.139438Z","shell.execute_reply.started":"2024-06-21T15:27:16.087248Z","shell.execute_reply":"2024-06-21T15:27:26.138318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Cell(nn.Module):\n    def __init__(self, *args, **kwargs) -> None:\n        '''\n        input: previous cell, input x_t\n        output: current cell\n        '''\n        super().__init__(*args, **kwargs)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.141210Z","iopub.execute_input":"2024-06-21T15:27:26.141783Z","iopub.status.idle":"2024-06-21T15:27:26.147455Z","shell.execute_reply.started":"2024-06-21T15:27:26.141753Z","shell.execute_reply":"2024-06-21T15:27:26.146295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTMCell(Cell):\n    def __init__(self,input_size, hidden_size, bias, drop_prob, device):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        self.linear_xf = nn.Linear(input_size, hidden_size, bias)\n        self.linear_hf = nn.Linear(hidden_size, hidden_size, bias)\n        \n        self.linear_xg = nn.Linear(input_size, hidden_size, bias)\n        self.linear_hg = nn.Linear(hidden_size, hidden_size, bias)\n        \n        self.linear_xi = nn.Linear(input_size, hidden_size, bias)\n        self.linear_hi = nn.Linear(hidden_size, hidden_size, bias)\n        \n        self.linear_xo = nn.Linear(input_size, hidden_size, bias)\n        self.linear_ho = nn.Linear(hidden_size, hidden_size, bias)\n        \n        self.sigmoid = nn.Sigmoid()\n        self.tanh = nn.Tanh()\n\n        self.drop_out = nn.Dropout(drop_prob)\n        self.device = device\n    def forward(self, xt, pre_ct = None, pre_ht = None):# pre_ht = h_{t-1}, pre_ct = c_{t-1}\n        # input size pre_ct, pre_ht: (batch_size, hidden_size)\n        #            xt: (batch_size, input_size)\n        # output size ct, ht: (batch_size, hidden_size)\n        if pre_ht == None:\n            pre_ht = torch.zeros(xt.size(0), self.hidden_size, device = self.device)\n        if pre_ct == None:\n            pre_ct = torch.zeros(xt.size(0), self.hidden_size, device = self.device)\n        \n        ft = self.sigmoid(\n                self.drop_out(self.linear_hf(pre_ht)) + \n                self.drop_out(self.linear_xf(xt))\n            )\n        \n        kt = pre_ct * ft\n        \n        gt = self.tanh(\n            self.drop_out(self.linear_hg(pre_ht)) +\n            self.drop_out(self.linear_xg(xt))\n        )\n        \n        it = self.sigmoid(\n            self.drop_out(self.linear_hi(pre_ht)) + \n            self.drop_out(self.linear_xi(xt))\n        )\n        \n        jt = gt * it\n        \n        ct = jt + kt\n        \n        ot = self.sigmoid(\n            self.drop_out(self.linear_ho(pre_ht)) + \n            self.drop_out(self.linear_xo(xt))\n        )\n        \n        ht = ot * self.tanh(ct)\n        \n        return ct, ht    \n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.149059Z","iopub.execute_input":"2024-06-21T15:27:26.149430Z","iopub.status.idle":"2024-06-21T15:27:26.167781Z","shell.execute_reply.started":"2024-06-21T15:27:26.149401Z","shell.execute_reply":"2024-06-21T15:27:26.166674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LayerLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, drop_prob, device, bidirectional = True, bias = True) -> None:\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bidirectional = bidirectional\n        self.bias = bias\n        self.device = device\n\n        if bidirectional:\n            if hidden_size % 2 != 0:\n                raise ValueError(\"When bidirectional is True, hidden size must be even\")\n            else:\n                self.hidden_size = int(self.hidden_size / 2)\n            \n            self.cellback = LSTMCell(self.input_size, self.hidden_size, self.bias, drop_prob, device)\n        \n        self.cell = LSTMCell(self.input_size, self.hidden_size, self.bias, drop_prob, device)\n\n    def forward(self, X, c = None):\n        '''\n        X: batch_size * N * embedding_dim\n        '''\n        # shift right\n        hidden_forward_c = [torch.zeros(X.size(0), self.hidden_size).to(self.device) for t in range(X.size(1))]\n        hidden_forward_h = [torch.zeros(X.size(0), self.hidden_size).to(self.device) for t in range(X.size(1))]\n\n        if c is None:\n            c0 = torch.zeros(X.size(0), self.hidden_size).to(self.device)\n            h0 = torch.zeros(X.size(0), self.hidden_size).to(self.device)\n        else:\n            c0 = c\n            h0 = c\n\n        for t in range(X.size(1)):\n            if t == 0:\n                hidden_forward_c[t], hidden_forward_h[t] = self.cell(X[:, t, :], c0, h0)\n            else:\n                hidden_forward_c[t], hidden_forward_h[t] = self.cell(\n                    X[:, t, :], hidden_forward_c[t-1], hidden_forward_h[t-1]\n                )\n        \n        hidden_backward_c = [torch.zeros(X.size(0), self.hidden_size).to(self.device) for t in range(X.size(1))]\n        hidden_backward_h = [torch.zeros(X.size(0), self.hidden_size).to(self.device) for t in range(X.size(1))]\n\n        if self.bidirectional:\n            for t in range(-1, -(X.size(1) + 1), -1):\n                if t == 0:\n                    hidden_backward_c[t], hidden_backward_h[t] = self.cellback(X[:, t, :], c0, h0)\n                else:\n                    hidden_backward_c[t], hidden_backward_h[t] = self.cellback(\n                        X[:, t, :], hidden_backward_c[t+1], hidden_backward_h[t+1]\n                    )\n\n            hidden_forward = torch.stack(hidden_forward_h, dim = 1)\n            hidden_backward = torch.stack(hidden_backward_h, dim = 1)\n\n            hidden_h = torch.concatenate((hidden_forward, hidden_backward), -1)\n            \n            return hidden_h\n\n        else:\n            hidden_h = torch.stack(hidden_forward_h, dim = 1)\n            return hidden_h\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.170424Z","iopub.execute_input":"2024-06-21T15:27:26.170722Z","iopub.status.idle":"2024-06-21T15:27:26.188208Z","shell.execute_reply.started":"2024-06-21T15:27:26.170698Z","shell.execute_reply":"2024-06-21T15:27:26.187207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_size, hidden_size, n_layers, drop_prob, vocab_size, device, bidirectional = True, bias = True) -> None:\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.bidirectional = bidirectional\n        self.embedd = nn.Embedding(vocab_size, input_size)\n        self.device = device\n        \n        self.D = 2 if bidirectional else 1\n\n        self.layer_0 = LayerLSTM(self.input_size, self.hidden_size, drop_prob, device,bidirectional, bias)\n\n        self.layers = nn.ModuleList(\n            [LayerLSTM(\n                self.hidden_size, self.hidden_size, drop_prob, device ,bidirectional, bias\n            ) for i in range(self.n_layers - 1)]\n        )\n\n    def forward(self, X):\n        out = self.embedd(X)\n        out = self.layer_0(out)\n        for layer_idx in range(self.n_layers - 1):\n            out = self.layers[layer_idx](out)\n        \n        out_forward = out[:, -1, :]\n\n        return out_forward\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.189286Z","iopub.execute_input":"2024-06-21T15:27:26.189562Z","iopub.status.idle":"2024-06-21T15:27:26.199466Z","shell.execute_reply.started":"2024-06-21T15:27:26.189528Z","shell.execute_reply":"2024-06-21T15:27:26.198602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, input_size, hidden_size, n_layers, drop_prob, vocab_size, deivce,bias = True) -> None:\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.embedd = nn.Embedding(vocab_size, input_size)\n        self.device = device\n\n        self.layer_0 = LayerLSTM(self.input_size, self.hidden_size, drop_prob, device,False, bias)\n\n        self.layers = nn.ModuleList(\n            [LayerLSTM(\n                self.hidden_size, self.hidden_size, drop_prob, device,False, bias\n            ) for i in range(self.n_layers - 1)]\n        )\n        self.output = nn.Linear(self.hidden_size, vocab_size, bias)\n        self.drop_out = nn.Dropout(drop_prob)\n\n    def forward(self, X, c):\n        out = self.embedd(X)\n        out = self.layer_0(out, c)\n        for layer_idx in range(self.n_layers - 1):\n            out = self.layers[layer_idx](out, c)\n\n        output = self.drop_out(self.output(out))\n\n        return output\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.200616Z","iopub.execute_input":"2024-06-21T15:27:26.202379Z","iopub.status.idle":"2024-06-21T15:27:26.214045Z","shell.execute_reply.started":"2024-06-21T15:27:26.202353Z","shell.execute_reply":"2024-06-21T15:27:26.212599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, input_size, hidden_size, n_layers, drop_prob, en_vocab_size, de_vocab_size, device,bias = True):\n        super().__init__()\n        self.encoder = Encoder(input_size, hidden_size, n_layers, drop_prob, en_vocab_size, device,True, bias)\n        self.decoder = Decoder(input_size, hidden_size, n_layers, drop_prob, de_vocab_size, device, bias)\n\n    def forward(self, src, trg):\n        enc = self.encoder(src)\n        dec = self.decoder(trg, enc)\n        return dec\n            ","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.215176Z","iopub.execute_input":"2024-06-21T15:27:26.215694Z","iopub.status.idle":"2024-06-21T15:27:26.226009Z","shell.execute_reply.started":"2024-06-21T15:27:26.215661Z","shell.execute_reply":"2024-06-21T15:27:26.225106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# GPU device setting\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# model parameter setting\n\n# optimizer parameter setting\ninit_lr = 1e-5\nfactor = 0.9\nadam_eps = 5e-9\npatience = 10\nwarmup = 100\nepoch = 500\nclip = 1.0\nweight_decay = 5e-4\ninf = float('inf')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.227128Z","iopub.execute_input":"2024-06-21T15:27:26.227391Z","iopub.status.idle":"2024-06-21T15:27:26.235279Z","shell.execute_reply.started":"2024-06-21T15:27:26.227369Z","shell.execute_reply":"2024-06-21T15:27:26.234421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def choice_sample_p(data, size):\n    np.random.seed(2206)\n    sample_p_idx = np.random.choice(np.arange(len(data)), size = size, replace= False)\n    sample_p_choice = np.isin(np.arange(len(data)), sample_p_idx)\n    sample_p_data = np.array(data)[sample_p_choice]\n    return sample_p_data\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\ndef get_bleu(hypotheses, reference):\n    \"\"\"Get validation BLEU score for dev set.\"\"\"\n    smoothie = SmoothingFunction().method1\n    bleu_score = sentence_bleu([reference], hypotheses, smoothing_function=smoothie)*100\n    return bleu_score\n\ndef idx_to_word(x, vocab):\n    words = []\n    for i in x:\n        word = vocab.get_itos()[i]\n        if '<' not in word:\n            words.append(word)\n    words = \" \".join(words)\n    return words","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.236441Z","iopub.execute_input":"2024-06-21T15:27:26.236720Z","iopub.status.idle":"2024-06-21T15:27:26.245509Z","shell.execute_reply.started":"2024-06-21T15:27:26.236696Z","shell.execute_reply":"2024-06-21T15:27:26.244702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eos_token = \"<eos>\"\nsos_token = \"<sos>\"\npad_token = \"<pad>\"\nunk_token = \"<unk>\"\nmax_length = 40\nmin_freq = 2\nbatch_size = 128\n\n# đọc file trainen, trainvi, tạo thành các list\ndef load_file(file_path):\n    sentences = []\n    with open(file_path, 'r') as file:\n        content = file.read()\n        sentences = content.split('\\n')\n    new_sentences = []\n    for sentence in sentences:\n        tokens = []\n        for token in sentence.split():\n            if token not in punctuation:\n                tokens.append(token)\n        new_sentence = ' '.join(tokens)\n        new_sentences.append(new_sentence)\n    return new_sentences\n\ndef tokenizer(sentences, lower, eos_token, sos_token, max_length):\n    return [sos_token] + html.unescape(\n        sentences.lower() if lower else sentences\n    ).split(' ')[:(max_length - 2)] + [eos_token]\n\n# train\ntrain_en_file = \"/kaggle/input/data-machine-translation/data/train/train.en\"\ntrain_vi_file = \"/kaggle/input/data-machine-translation/data/train/train.vi\"\n\ntrain_en = load_file(train_en_file)\ntrain_vi = load_file(train_vi_file)\n\ntrain_en_tokens = [tokenizer(sent,True, eos_token, sos_token, max_length) for sent in train_en]\ntrain_vi_tokens = [tokenizer(sent,True, eos_token, sos_token, max_length) for sent in train_vi]\n\n# test\ntest_en_file = \"/kaggle/input/data-machine-translation/data/test/train.en\"\ntest_vi_file = \"/kaggle/input/data-machine-translation/data/test/train.vi\"\n\ntest_en = load_file(test_en_file)\ntest_vi = load_file(test_vi_file)\n\ntest_en_tokens = [tokenizer(sent,True, eos_token, sos_token, max_length) for sent in test_en]\ntest_vi_tokens = [tokenizer(sent,True, eos_token, sos_token, max_length) for sent in test_vi]\n\n#dev\ndev_en_file = \"/kaggle/input/data-machine-translation/data/dev/train.en\"\ndev_vi_file = \"/kaggle/input/data-machine-translation/data/dev/train.vi\"\n\ndev_en = load_file(dev_en_file)\ndev_vi = load_file(dev_vi_file)\n\ndev_en_tokens = [tokenizer(sent,True, eos_token, sos_token, max_length) for sent in dev_en]\ndev_vi_tokens = [tokenizer(sent,True, eos_token, sos_token, max_length) for sent in dev_vi]\n\n# vocab\nspecial_tokens = [\n    unk_token,\n    pad_token,\n    sos_token,\n    eos_token\n]\nen_vocab = torchtext.vocab.build_vocab_from_iterator(\n    train_en_tokens, min_freq,special_tokens,\n)\nvi_vocab = torchtext.vocab.build_vocab_from_iterator(\n    train_vi_tokens, min_freq, special_tokens\n)\n\nassert en_vocab[unk_token] == vi_vocab[unk_token]\nassert en_vocab[pad_token] == vi_vocab[pad_token]\n\nunk_index = en_vocab[unk_token]\npad_index = en_vocab[pad_token]\n\nen_vocab.set_default_index(unk_index)\nvi_vocab.set_default_index(unk_index)\n\nen_vocab_size = len(en_vocab)\nvi_vocab_size = len(vi_vocab)\n\n#train\ntrain_en_ids = [en_vocab.lookup_indices(toks) for toks in train_en_tokens]\ntrain_vi_ids = [vi_vocab.lookup_indices(toks) for toks in train_vi_tokens]\n\n#test\ntest_en_ids = [en_vocab.lookup_indices(toks) for toks in test_en_tokens]\ntest_vi_ids = [vi_vocab.lookup_indices(toks) for toks in test_vi_tokens]\n\n#dev\ndev_en_ids = [en_vocab.lookup_indices(toks) for toks in dev_en_tokens]\ndev_vi_ids = [vi_vocab.lookup_indices(toks) for toks in dev_vi_tokens]\n\nclass Bitext(Dataset):\n    def __init__(self, src, trg) -> None:\n        super().__init__()\n        self.src = src\n        self.trg = trg\n    def __len__(self):\n        return len(self.src)\n    def __getitem__(self, index):\n        return self.src[index], self.trg[index]\n    \ntrain_dataset = Bitext(train_en_ids, train_vi_ids)\ntest_dataset = Bitext(test_en_ids, test_vi_ids)\ndev_dataset = Bitext(dev_en_ids, dev_vi_ids)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.249248Z","iopub.execute_input":"2024-06-21T15:27:26.249546Z","iopub.status.idle":"2024-06-21T15:27:26.731748Z","shell.execute_reply.started":"2024-06-21T15:27:26.249524Z","shell.execute_reply":"2024-06-21T15:27:26.730708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_collate_fn(pad_index):\n    def collate_fn(batch):\n        src, trg = zip(*batch)\n\n        src = [torch.LongTensor(toks) for toks in src]\n        trg = [torch.LongTensor(toks) for toks in trg]\n        \n        src = nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=pad_index)\n        trg = nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=pad_index)\n\n        return src, trg\n\n    return collate_fn\n\ndef get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n    collate_fn = get_collate_fn(pad_index)\n    data_loader = torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        collate_fn=collate_fn,\n        shuffle=shuffle\n    )\n    return data_loader\n\ntrain_dataloader = get_data_loader(\n    train_dataset, batch_size, pad_index, True\n)\n\ntest_dataloader = get_data_loader(\n    test_dataset, batch_size, pad_index, True\n)\n\ndev_dataloader = get_data_loader(\n    dev_dataset, batch_size, pad_index, True\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.732983Z","iopub.execute_input":"2024-06-21T15:27:26.733377Z","iopub.status.idle":"2024-06-21T15:27:26.742789Z","shell.execute_reply.started":"2024-06-21T15:27:26.733343Z","shell.execute_reply":"2024-06-21T15:27:26.741828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef initialize_weights(m):\n    if hasattr(m, 'weight') and m.weight.dim() > 1:\n        nn.init.kaiming_uniform(m.weight.data)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.743799Z","iopub.execute_input":"2024-06-21T15:27:26.744094Z","iopub.status.idle":"2024-06-21T15:27:26.756406Z","shell.execute_reply.started":"2024-06-21T15:27:26.744046Z","shell.execute_reply":"2024-06-21T15:27:26.755409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_size = 128\nhidden_size = 256\nn_layers = 2 #sửa lại: 4\ndrop_prob = 0.1\nmodel = Seq2Seq(\n    input_size=input_size,\n    hidden_size=hidden_size,\n    n_layers=n_layers,\n    drop_prob=drop_prob,\n    en_vocab_size=en_vocab_size,\n    de_vocab_size= vi_vocab_size,\n    device = device\n).to(device)\nprint(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.757414Z","iopub.execute_input":"2024-06-21T15:27:26.757692Z","iopub.status.idle":"2024-06-21T15:27:26.963629Z","shell.execute_reply.started":"2024-06-21T15:27:26.757669Z","shell.execute_reply":"2024-06-21T15:27:26.962647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.apply(initialize_weights)\noptimizer = Adam(params=model.parameters(),\n                 lr=init_lr,\n                 weight_decay=weight_decay,\n                 eps=adam_eps)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:26.964657Z","iopub.execute_input":"2024-06-21T15:27:26.964948Z","iopub.status.idle":"2024-06-21T15:27:27.738555Z","shell.execute_reply.started":"2024-06-21T15:27:26.964922Z","shell.execute_reply":"2024-06-21T15:27:27.737660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n                                                 verbose=True,\n                                                 factor=factor,\n                                                 patience=patience)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_index)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:27.739661Z","iopub.execute_input":"2024-06-21T15:27:27.740203Z","iopub.status.idle":"2024-06-21T15:27:27.747980Z","shell.execute_reply.started":"2024-06-21T15:27:27.740175Z","shell.execute_reply":"2024-06-21T15:27:27.747016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion, clip, device):\n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(iterator):\n        src, trg = batch\n        src = src.to(device)\n        trg = trg.to(device)\n\n        optimizer.zero_grad()\n        output = model(src, trg[:, :-1])\n        output_reshape = output.contiguous().view(-1, output.shape[-1])\n        trg = trg[:, 1:].contiguous().view(-1)\n\n        loss = criterion(output_reshape, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n\n    return epoch_loss / len(iterator)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:27.749231Z","iopub.execute_input":"2024-06-21T15:27:27.749505Z","iopub.status.idle":"2024-06-21T15:27:27.759114Z","shell.execute_reply.started":"2024-06-21T15:27:27.749475Z","shell.execute_reply":"2024-06-21T15:27:27.758233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, iterator, criterion, device):\n    model.eval()\n    epoch_loss = 0\n    batch_bleu = []\n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n            src, trg = batch\n            src = src.to(device)\n            trg = trg.to(device)\n            \n            output = model(src, trg[:, :-1])\n            output_reshape = output.contiguous().view(-1, output.shape[-1])\n            trg = trg[:, 1:].contiguous().view(-1)\n\n            loss = criterion(output_reshape, trg)\n            epoch_loss += loss.item()\n\n            total_bleu = []\n\n            src, trg = batch\n            for j in range(len(trg)):\n                try:\n                    trg_words = idx_to_word(trg[j], vi_vocab)\n                    output_words = output[j].argmax(1)\n                    output_words = idx_to_word(output_words, vi_vocab)\n                    ble = get_bleu(hypotheses=output_words.split(), reference=trg_words.split())\n                    total_bleu.append(ble)\n                except:\n                    pass\n\n            batch_bleu.append(np.mean(total_bleu))\n\n    batch_bleu = np.mean(batch_bleu)\n    return epoch_loss / len(iterator), batch_bleu","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:27.760378Z","iopub.execute_input":"2024-06-21T15:27:27.760669Z","iopub.status.idle":"2024-06-21T15:27:27.771467Z","shell.execute_reply.started":"2024-06-21T15:27:27.760647Z","shell.execute_reply":"2024-06-21T15:27:27.770508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from util.util import *\ndef run(model, total_epoch, best_loss, device):\n    train_losses, test_losses, bleus = [], [], []\n    for step in range(total_epoch):\n        start_time = time.time()\n        train_loss = train(model, train_dataloader, optimizer, criterion, clip, device)\n        valid_loss, bleu = evaluate(model, dev_dataloader, criterion, device) # tính chỉ số bleu trên mỗi epoch\n        end_time = time.time()\n\n        if step > warmup:\n            scheduler.step(valid_loss)\n\n        train_losses.append(train_loss)\n        test_losses.append(valid_loss)\n        bleus.append(bleu)\n        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n#             torch.save(model.state_dict(), '/kaggle/working/model-{0}.pt'.format(valid_loss))\n        if step % 20 == 0:\n            torch.save(model.state_dict(), '/kaggle/working/model-{0}.pt'.format(step))\n\n        f = open('/kaggle/working/train_loss.txt', 'w') # lưu lại train_loss để vẽ đồ thị\n        f.write(str(train_losses))\n        f.close()\n\n        f = open('/kaggle/working/bleu.txt', 'w')\n        f.write(str(bleus))\n        f.close()\n\n        f = open('/kaggle/working/valid_loss.txt', 'w') # lưu lại valid_loss để vẽ đồ thị\n        f.write(str(test_losses))\n        f.close()\n\n        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n        print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {math.exp(valid_loss):7.3f}')\n        print(f'\\tBLEU Score: {bleu:.3f}')\n    # tính loss, bleu trên tập test:\n    test_loss, bleu_test = evaluate(model, test_dataloader, criterion, device)\n    print(f\"\\tTest Loss: {test_loss:.3f} | Bleu Score of Test: {bleu_test:.3f}\")\n    f = open('/kaggle/working/result_test_loss.txt', 'w')\n    f.write(str(test_loss))\n    f.close()\n\n    f = open('/kaggle/working/result_Bleu_Test.txt', 'w')\n    f.write(str(bleu_test))\n    f.close()\n#     return train_losses, test_losses, bleus\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:27.772883Z","iopub.execute_input":"2024-06-21T15:27:27.773175Z","iopub.status.idle":"2024-06-21T15:27:27.785888Z","shell.execute_reply.started":"2024-06-21T15:27:27.773152Z","shell.execute_reply":"2024-06-21T15:27:27.785145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epoch = 2\nrun(model = model,total_epoch = epoch, best_loss=inf, device = device)\ntorch.save(model.state_dict(), '/kaggle/working/model-official.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:27:27.786845Z","iopub.execute_input":"2024-06-21T15:27:27.787147Z","iopub.status.idle":"2024-06-21T15:28:28.239875Z","shell.execute_reply.started":"2024-06-21T15:27:27.787093Z","shell.execute_reply":"2024-06-21T15:28:28.237998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dịch: \nfor batch in test_dataloader:\n    src, trg = batch\n    break\nsrc = src.to(device)\ntrg = trg.to(device)\noutput = model(src, trg[:, :-1])\n\ntrg_words = idx_to_word(trg[0], vi_vocab)\noutput_words = output[0].argmax(1)\noutput_words = idx_to_word(output_words, vi_vocab)\nprint(f\"Bản gốc: {trg_words} \\n Bản dịch: {output_words}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T15:28:28.241781Z","iopub.execute_input":"2024-06-21T15:28:28.242443Z","iopub.status.idle":"2024-06-21T15:28:28.610152Z","shell.execute_reply.started":"2024-06-21T15:28:28.242402Z","shell.execute_reply":"2024-06-21T15:28:28.609051Z"},"trusted":true},"execution_count":null,"outputs":[]}]}